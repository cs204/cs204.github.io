{% extends "layout.html" %}

{% block title %}
Машинное обучение
{% endblock %}
{% block body %}
Оригинал на 
<a href="https://cs50.harvard.edu/ai/2023/notes/4/">cs50 AI</a>.
<h2>Машинное обучение</h2>
<hr>
<p>
Машинное обучение предоставляет компьютеру данные, 
а не явные инструкции. Используя эти данные, 
компьютер учится распознавать закономерности 
и может самостоятельно выполнять задачи.
</p>

<h2>Обучение с учителем</h2>
<p>Обучение с учителем — это задача, в которой компьютер 
обучает функцию, которая сопоставляет входные данные 
с выходными на основе набора данных пар ввода-вывода.
</p>

<p>В рамках контролируемого обучения существует множество задач, 
и одна из них — классификация.
Это задача, в которой функция сопоставляет вход с дискретным выходом.
Например, получив некоторую информацию о влажности и давлении 
воздуха за определённый день (входные данные), компьютер решает, 
будет ли в этот день дождь или нет (выходные данные).
Компьютер делает это после обучения на наборе данных 
с несколькими днями, где влажность и давление воздуха 
уже сопоставлены с тем, шёл дождь или нет.
</p>

<p>Эту задачу можно формализовать следующим образом.
Мы наблюдаем природу, где функция f(влажность, давление) 
отображает входные данные в дискретное значение: 
дождь или отсутствие дождя.
Эта функция скрыта от нас, и на неё, вероятно, влияют 
многие другие переменные, к которым у нас нет доступа.
Наша цель — создать функцию h(влажность, давление), 
которая может аппроксимировать поведение функции f.
Такую задачу можно визуализировать, отобразив дни по 
измерениям влажности и дождя (входные данные), раскрасив 
каждую точку данных синим цветом, если в тот день шёл дождь, 
и красным, если в тот день дождя не было (выходные данные).
Белая точка данных имеет только входные данные, 
а компьютеру необходимо определить выходные данные.
</p>
<figure class="figure">
	<img src="/cs23/data/images/classification.png" class="figure-img img-fluid">
</figure>

<h2>Классификация ближайших соседей</h2>
<p>
Одним из способов решения задачи, подобной описанной выше, 
является присвоение рассматриваемой переменной значения 
ближайшего наблюдения.
Так, например, белая точка на графике выше будет окрашена 
в синий цвет, поскольку ближайшая наблюдаемая точка тоже синяя.
Иногда это может сработать, но обратите внимание на график ниже.
</p>
<figure class="figure">
	<img src="/cs23/data/images/nearestneighbor.png" class="figure-img img-fluid">
</figure>
<p>
Следуя той же стратегии, белую точку следует покрасить 
в красный цвет, поскольку ближайшее к ней наблюдение 
тоже красного цвета.
Однако, если посмотреть на картину в целом, похоже, 
что большинство других наблюдений вокруг него синие, 
что может дать нам интуитивное представление о том, 
что синий цвет в данном случае является лучшим прогнозом, 
хотя самое близкое наблюдение — красное.
</p>

<p>
Один из способов обойти ограничения классификации 
ближайших соседей — использовать классификацию 
k-ближайших соседей, при которой точка окрашивается 
на основе наиболее часто встречающегося цвета из k 
ближайших соседей.
Программист должен решить, какое выбрать  k.
Например, при использовании классификации трёх ближайших 
соседей белая точка выше будет окрашена в синий цвет, 
что интуитивно кажется лучшим решением.
</p>

<p>
Недостаток классификации k-ближайших соседей заключается в том, 
что при использовании наивного подхода алгоритму придётся 
измерять расстояние каждой отдельной точки до рассматриваемой точки, 
что требует больших вычислительных затрат.
Это можно ускорить, используя структуры данных, которые позволяют 
быстрее находить соседей, или отсекая ненужные наблюдения.
</p>

<h2>Перцептронное обучение</h2>
<p>
Другой способ решения проблемы классификации, в отличие от 
стратегии ближайшего соседа, заключается в рассмотрении 
данных в целом и попытке создать границу решения.
В двумерных данных мы можем провести линию между двумя 
типами наблюдений.
Каждая дополнительная точка данных будет классифицироваться 
в зависимости от стороны линии, на которой она нанесена.
</p>
<figure class="figure">
	<img src="/cs23/data/images/decisionboundary.png" class="figure-img img-fluid">
</figure>
<p>
Недостаток этого подхода заключается в том, что данные беспорядочны, 
и редко удаётся провести линию и аккуратно разделить классы 
на два наблюдения без каких-либо ошибок.
Часто мы идём на компромисс, проводя границу, которая чаще 
всего правильно разделяет наблюдения, но все же иногда 
неправильно их классифицирует.
<p>В этом случае ввод:</p>
<ul>
	<li>
		\(x_1\)= Humidity
	</li>
	<li>
		\(x_2\)= Pressure
	</li>
</ul>
будет передан функции гипотезе \(h(x_1, x_2)\), которая выдаст 
прогноз того, будет ли в этот день дождь или нет.
Это будет сделано путём проверки, на какую сторону 
границы решения попадает наблюдение.
Формально, функция будет умножать каждое  из входных данных
на свой коэффициент (вес) и добавлять константу, запишем 
формулу:
<ul>
	<li>Дождь \(w_0+w_1x_1+w_2x_2\ge 0\)</li>
	<li>Нет дождя в противном случае</li>
</ul>
</p>

<p>
Часто выходная переменная кодируется как 1 и 0, где, 
если уравнение дает больше 0, выходной сигнал равен 1 (дождь), 
а в противном случае — 0 (нет дождя).
</p>
<p>Веса и значения представлены векторами, которые представляют 
собой последовательности чисел (которые в Python можно хранить 
в списках или кортежах).
Мы создаём весовой вектор w: \((w_0, w_1, w_2)\), и получение 
наилучшего весового вектора является целью алгоритма 
машинного обучения.
Мы также создаём входной вектор x: \((1, x_1, x_2)\).
</p>

<p>Мы берём скалярное произведение двух векторов.
То есть мы умножаем каждое значение в одном векторе 
на соответствующее значение во втором векторе, 
получая выражение выше: w₀ + w₁x₁ + w₂x₂.
Первое значение входного вектора равно 1, 
потому что при умножении на вектор веса w₀ мы 
хотим сохранить его постоянным.
</p>
<p>Таким образом, мы можем представить нашу гипотезную 
функцию следующим образом:
$$
h_w(x)=\left. \begin{array}{ll}
1 & if\; wx\ge0\\
0 & otherwise
\end{array}
\right.
$$
</p>
<p>Поскольку целью алгоритма является поиск наилучшего 
вектора весов, когда алгоритм встречает новые данные, 
он обновляет текущие веса.
Это делается с использованием правила обучения перцептрона:
</p>
$$
w_i=w_i+\alpha(y-h_w(x))x_i
$$
<p>Важным выводом из этого правила является то, что для каждой 
точки данных мы корректируем веса, чтобы сделать нашу функцию 
более точной.
Детали, которые с нашей точки зрения не так важны, 
заключаются в том, что каждый вес устанавливается равным 
самому себе плюс некоторое значение в скобках.
Здесь y обозначает наблюдаемое значение, а функция гипотезы 
обозначает оценку.
Если они идентичны, весь этот член равен нулю, и, 
следовательно, вес не изменяется.
Если мы недооценили (вызов «Нет дождя, пока наблюдался дождь»), 
то значение в скобках будет равно 1, а вес увеличится на значение xᵢ, 
масштабированное с помощью коэффициента обучения α.
Если мы переоценили (вызов Rain при отсутствии дождя), 
то значение в скобках будет -1, а вес уменьшится на значение x, 
масштабированное по α.
Чем выше α, тем сильнее влияние каждого нового события на вес.
</p>
<p>Результатом этого процесса является пороговая функция, 
которая переключается с 0 на 1, как только оценочное 
значение пересекает некоторый порог.
</p>
<figure class="figure">
	<img src="/cs23/data/images/hardthreshold.png" class="figure-img img-fluid">
</figure>
<p>
Проблема с этим типом функции заключается в том, что 
она не может выражать неопределенность, поскольку может быть 
равна только 0 или 1.
Она использует жесткий порог.
Чтобы обойти эту проблему, можно использовать логистическую функцию, 
которая использует мягкий порог.
Логистическая функция может давать действительное число от 0 до 1, 
что будет выражать уверенность в оценке.
Чем ближе значение к 1, тем больше вероятность дождя.
</p>
<figure class="figure">
	<img src="/cs23/data/images/softthreshold.png" class="figure-img img-fluid">
</figure>

<h2>Машины опорных векторов</h2>
<p>Помимо метода ближайшего соседа и линейной регрессии, 
еще одним подходом к классификации является машина опорных векторов.
Этот подход использует дополнительный вектор (вектор поддержки) 
рядом с границей решения, чтобы принять лучшее решение при разделении данных.
Рассмотрим пример ниже.
</p>
<figure class="figure">
	<img src="/cs23/data/images/supportvector.png" class="figure-img img-fluid">
</figure>
<p>Все границы принятия решений работают таким образом, 
что разделяют данные без каких-либо ошибок.
Однако одинаково ли они хороши?
Две крайние левые границы решения очень близки к некоторым наблюдениям.
Это означает, что новая точка данных, лишь незначительно отличающаяся 
от одной группы, может быть ошибочно отнесена к другой.
Напротив, крайняя правая граница решений находится на наибольшем 
расстоянии от каждой из групп, что дает наибольшую свободу действий внутри нее.
Этот тип границы, который находится как можно дальше от двух групп, которые он разделяет, называется разделителем максимального поля.
</p>
<p>
Еще одним преимуществом машин опорных векторов является то, 
что они могут представлять границы решений с более чем двумя измерениями, 
а также нелинейные границы решений, как показано ниже.
</p>
<figure class="figure">
	<img src="/cs23/data/images/circleboundary.png" class="figure-img img-fluid">
</figure>
<p>
Подводя итог, можно сказать, что существует множество способов 
решения задач классификации, и нет такого, который всегда лучше других.
Каждый из них имеет свои недостатки и может оказаться более полезным, 
чем другие, в конкретных ситуациях.
</p>

<h2>Регрессия</h2>
<p>
Регрессия — это задача обучения с учителем функции, 
которая сопоставляет входную точку с непрерывным значением, 
некоторым действительным числом.
Это отличается от классификации тем, что задачи классификации 
отображают входные данные в дискретные значения (дождь или отсутствие дождя).
</p>

<p>Например, компания может использовать регрессию, чтобы ответить 
на вопрос, как деньги, потраченные на рекламу, предсказывают деньги, 
полученные от продаж.
В этом случае наблюдаемая функция f(реклама) представляет 
собой наблюдаемый доход после некоторой суммы денег, потраченных 
на рекламу (обратите внимание, что функция может принимать 
более одной входной переменной).
Это данные, с которых мы начинаем. 
Используя эти данные, мы хотим придумать гипотезу функции h (реклама), 
которая попытается аппроксимировать поведение функции f. 
h сгенерирует линию, целью которой является не разделение 
типов наблюдений, а предсказание на основе входных данных того, 
каким будет значение выходных данных.
</p>
<figure class="figure">
	<img src="/cs23/data/images/regression.png" class="figure-img img-fluid">
</figure>

<h2>Функция потерь</h2>
<p>
Функции потерь — это способ количественной оценки полезности, 
потерянной в результате любого из приведенных выше правил 
принятия решений.
Чем менее точный прогноз, тем больше потери.
</p>
<p>Для задач классификации мы можем использовать функцию потерь 0-1.
</p>
L(действительное, предсказанное)
<ul>
	<li>0 если действительное = предсказанное</li>
	<li>1 в других случаях</li> 
</ul>
<p>Другими словами, эта функция получает значение, 
когда прогноз неверен, и не получает значения, когда он верен 
(т. е. когда наблюдаемые и прогнозируемые значения совпадают).
</p>

<figure class="figure">
	<img src="/cs23/data/images/01loss.png" class="figure-img img-fluid">
</figure>
<p>

</p>
{% endblock %}


