{% extends "layout.html" %}
{% block title %}
Nim
{% endblock %}
{% block body %}
<h1>Ним</h1>
<p>Напишите ИИ, который учится играть в Нима 
посредством обучения с подкреплением.
</p>
<pre>
<code>
$python play.py
Playing training game 1
Playing training game 2


</code>
</pre>
<h2>Пояснение</h2>
<p>
Напомним, что в игре Ним мы начинаем с некоторого количества стопок, 
в каждой из которых находится определённое количество предметов.
Игроки ходят по очереди: в свой ход игрок убирает любое 
неотрицательное количество предметов из любой непустой стопки.
Проигрывает тот, кто уберут последний предмет.
</p>

<p>
Для этой игры вы можете придумать простую стратегию: 
если в ней осталась только одна стопка и три объекта, 
и настал ваш ход, лучше всего удалить два из этих объектов, 
оставив оппоненту третий и последний объект, который нужно удалить.
Но если стопок больше, стратегия значительно усложняется.
В этой задаче мы создадим ИИ, который будет изучать 
стратегию этой игры посредством обучения с подкреплением.
Постоянно играя против самого себя и извлекая уроки из опыта, 
в конечном итоге наш ИИ научится, какие действия предпринимать, 
а каких избегать.
</p>

<p>
В частности, для этого проекта мы будем использовать Q-learning.
Напомним, что в Q-обучении мы пытаемся узнать значение 
вознаграждения (число) для каждой пары (состояние, действие).
Действие, которое приводит к проигрышу игры, будет иметь награду -1, 
действие, которое приводит к проигрышу игры другим игроком, 
будет иметь награду 1, а действие, которое приводит к продолжению игры, 
имеет немедленную награду 0, но также будет иметь некоторую будущую награду.
</p>

<p>Как мы будем представлять состояния и действия внутри программы Python?
«Состояние» игры «Ним» — это текущий размер всех стопок.
Например, состоянием может быть [1, 1, 3, 5], представляющее 
состояние с 1 объектом в стопке 0, 1 объектом в стопке 1, 3 объектами в стопке 
2 и 5 объектами в стопке 3.
«Действием» в игре Ним будет пара целых чисел (i, j), 
представляющая действие по взятию j предметов из стопки i.
Таким образом, действие (3, 5) представляет собой действие 
«из стопки 3 забрать 5 предметов».
Применение этого действия к состоянию [1, 1, 3, 5] приведёт к 
созданию нового состояния [1, 1, 3, 0] 
(то же самое состояние, но стопка 3 теперь пуста).
</p>

<p>Напомним, что ключевая формула Q-обучения приведена ниже.
Каждый раз, когда мы находимся в состоянии s и выполняем действие a, 
мы можем обновить значение Q Q(s, a) согласно:
</p>
<pre>
Q(s, a) <- Q(s, a) + alpha * (new value estimate - old value estimate)
</pre>
<p>В приведённой выше формуле alpha — это скорость обучения 
(насколько мы ценим новую информацию по сравнению с информацией, 
которая у нас уже есть).
Новая оценка значения представляет собой сумму награды, полученной 
за текущее действие, и оценку всех будущих наград, которые получит игрок.
Старая оценка значения — это просто существующее значение для Q(s, a).
Применяя эту формулу каждый раз, когда наш ИИ предпринимает новое действие, 
со временем наш ИИ начнёт узнавать, какие действия лучше в том или ином состоянии.
</p>
<h2>Начало</h2>
<p>Загрузите код и распакуйте.
<pre>
<code>
wget "https://vadimgb.github.io/psets/data/nim.zip"
unzip nim.zip
cd uim
</code>
</pre>
<h2>Понимание</h2>
<p>
Сначала откройте nim.py.
В этом файле определены два класса (Nim и NimAI) и две функции 
(обучение и игра).
Nim, Train и Play уже реализованы за вас, а NimAI оставляет 
вам несколько функций для реализации.
</p>

<p>Взгляните на класс Nim, который определяет, как ведётся игра Nim.
Обратите внимание, что в функции __init__ каждая игра Nim должна отслеживать 
список стопок, текущего игрока (0 или 1) и победителя игры 
(если таковой существует).
Функция available_actions возвращает набор всех доступных действий в состоянии.
Например, Nim.available_actions([2, 1, 0, 0]) возвращает набор 
{(0, 1), (1, 1), (0, 2)}, поскольку три возможных действия должны 
выполнить либо 1 или 2 предмета из стопки 0, или взять 1 предмет из стопки 1.
</p>

<p>
Остальные функции используются для определения игрового процесса: 
функция other_player определяет, кто является противником данного игрока, 
switch_player меняет текущего игрока на игрока противника, 
а move выполняет действие в текущем состоянии и переключает 
текущего игрока на игрока противника. 
</p>
<p>
Далее взгляните на класс NimAI, который определяет наш ИИ, 
который будет учиться играть в Ним.
Обратите внимание, что в функции __init__ мы начинаем с пустого словаря self.q.
Словарь self.q будет отслеживать все текущие значения Q, 
полученные нашим ИИ, путём сопоставления пар (состояние, действие) 
с числовыми значениями.
В качестве детали реализации, хотя мы обычно представляем состояние 
в виде списка, поскольку списки не могут использоваться в качестве 
ключей словаря Python, вместо этого мы будем использовать кортежную 
версию состояния при получении или установке значений в self.q.
</p>
<p>Например, если бы мы хотели установить значение 
Q состояния [0, 0, 0, 2] и действия (3, 2) равным 
-1, мы бы написали что-то вроде
</p>
<pre>
<code class="language-python">
self.q[(0, 0, 0, 2), (3, 2)] = -1
</code>
</pre>
<p>
Также обратите внимание, что каждый объект NimAI имеет 
значения alfa и epsilon, которые будут использоваться 
для Q-обучения и выбора действия соответственно.
</p>
<p>Функция update написана для вас и принимает 
в качестве входных данных состояние old_state, действие, 
выполненное в этом состоянии, результирующее состояние 
после выполнения этого действия new_state 
и немедленное вознаграждение за выполнение этого действия.
Затем функция выполняет Q-обучение, сначала получая 
текущее значение Q для состояния и действия 
(путём вызова get_q_value), определяя наилучшие возможные 
будущие награды (путём вызова best_future_reward), 
а затем используя оба этих значения для обновления 
Q-значения (путём вызова update_q_value).
Эти три функции  вам надо определить.
</p>
<p>Наконец, последняя функция, оставшаяся нереализованной, — это 
функция choose_action, которая выбирает действие, которое 
необходимо выполнить в заданном состоянии (либо жадно, 
либо с использованием эпсилон-жадного алгоритма).
</p>
<p>
Классы Nim и NimAI в конечном итоге используются 
в функциях train и play.
Функция train тренирует ИИ, запуская против него самого n 
симулированных игр и возвращая полностью обученный ИИ.
Функция play принимает на вход обученный ИИ и 
позволяет игроку-человеку сыграть в игру Ним против ИИ.
</p>
<h2>Спецификация</h2>
<p>Завершите реализацию get_q_value, update_q_value, 
best_future_reward и choose_action в nim.py.
Для каждой из этих функций каждый раз, когда функция 
принимает состояние в качестве входных данных, вы можете 
предположить, что это список целых чисел.
Каждый раз, когда функция принимает действие в 
качестве входных данных, вы можете предположить, 
что это целочисленная пара (i, j) стопки i и количества объектов j.
</p>
<p>Функция get_q_value должна принимать в качестве входных 
данных состояние и действие и возвращать соответствующее 
значение Q для этой пары состояние/действие.
</p>
<ul>
	<li> 
		Напомним, что Q-значения хранятся в словаре self.q.  
		Ключи self.q должны быть в виде пар 
		(состояние, действие), где состояние — это кортеж 
		всех размеров стопок по порядку, а действие — это 
		кортеж (i, j), представляющий стопку и число.
	</li>
	<li> 
		Если в self.q не существует значения Q для пары 
		состояние/действие, функция должна вернуть 0.
	</li>
</ul>
<p>
Функция update_q_value принимает состояние состояния, 
действие действие, существующее значение Q old_q, 
текущее вознаграждение и оценку будущих вознаграждений 
future_rewards, и обновляет значение Q для пары 
состояние/действие в соответствии с формулой Q-обучения. .
</p>
<ul>
	<li>
		Напомним, что формула Q-обучения 
		выглядит следующим образом: 
		<pre> 
		<code> 
      Q(s, a) <- старая оценка значения + альфа * (новая оценка значения - старая оценка значения)
      </code>
		</pre>
	</li>
	<li> 
		Напомним, что альфа — это скорость обучения, 
		связанная с объектом NimAI.
	</li>
	<li> 
		Старая оценка значения — это просто существующее 
		значение Q для пары состояние/действие.  
		Новая оценка стоимости должна представлять собой 
		сумму текущего вознаграждения и предполагаемого 
		будущего вознаграждения.
	</li>
</ul>
<p>Функция best_future_reward принимает состояние в качестве 
входных данных и возвращает наилучшую возможную награду 
за любое доступное действие в этом состоянии, согласно 
данным в self.q.
</p>
<ul>
	<li>
		Для любого действия, которого ещё нет в self.q для 
		данного состояния, следует предположить, что его 
		значение Q равно 0.
	</li>
	<li>
		Если в состоянии нет доступных действий, 
		вы должны вернуть 0.
	</li>
</ul>
<p>
Функция choose_action должна принимать состояние в качестве 
входных данных (и, возможно, флаг эпсилон, указывающий, 
следует ли использовать эпсилон-жадный алгоритм) и возвращать 
доступное действие в этом состоянии.
</p>
<ul>
	<li> 
		Если epsilon имеет значение False, ваша функция 
		должна вести себя жадно и возвращать наилучшее 
		возможное действие, доступное в этом состоянии 
		(т. е. действие, которое имеет наибольшее 
		значение Q, используя 0, если значение Q не известно).
	</li>
	<li> 
		Если epsilon имеет значение True, ваша функция 
		должна вести себя в соответствии с эпсилон-жадным 
		алгоритмом, выбирая случайное доступное действие 
		с вероятностью self.epsilon и в противном случае 
		выбирая лучшее доступное действие.
	</li>
	<li> 
		Если несколько действий имеют одно и то же 
		значение Q, любая из этих опций является 
		приемлемым возвращаемым значением.
	</li>
</ul>
<p>
Вам не следует изменять что-либо ещё в nim.py, кроме функций, 
которые спецификация требует от вас реализовать, хотя вы можете 
писать дополнительные функции и/или импортировать другие модули 
стандартной библиотеки Python.
Вы также можете импортировать numpy или pandas, если знакомы 
с ними, но вам не следует использовать какие-либо другие 
сторонние модули Python.
Вы можете изменить play.py для самостоятельного тестирования.
</p>
{% endblock %}

